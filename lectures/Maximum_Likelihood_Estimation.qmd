---
title: "Maximum Likelihood Estimation"
author: "Linear Regression Course"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: false
    theme: cosmo
    embed-resources: true
---

## Introduction to Maximum Likelihood Estimation

Maximum Likelihood Estimation (MLE) is a fundamental statistical method for estimating the parameters of a probability distribution by maximizing a likelihood function. The core idea is to find parameter values that make the observed data most probable.

### The Fundamental Principle

Given observed data $y_1, y_2, \ldots, y_n$ and a statistical model with parameters $\theta$, MLE seeks to find the parameter values $\hat{\theta}$ that maximize the probability (or likelihood) of observing the actual data.

**Key Question**: What parameter values make our observed data most likely to have occurred?

## The Likelihood Function

### Definition

For independent observations $y_1, y_2, \ldots, y_n$ from a probability density function $f(y|\theta)$, the likelihood function is:

$$
L(\theta | y_1, \ldots, y_n) = \prod_{i=1}^{n} f(y_i | \theta)
$$

**Important Distinction**: 
- **Probability**: $f(y|\theta)$ - fixes $\theta$, varies $y$
- **Likelihood**: $L(\theta|y)$ - fixes $y$, varies $\theta$

### Log-Likelihood

In practice, we often work with the log-likelihood function:

$$
\ell(\theta) = \log L(\theta) = \sum_{i=1}^{n} \log f(y_i | \theta)
$$

**Why use log-likelihood?**
1. Converts products to sums (easier computation)
2. Avoids numerical underflow
3. Monotonic transformation preserves the maximum

## Finding the Maximum Likelihood Estimator

### General Approach

1. Write down the likelihood function $L(\theta | \text{data})$
2. Take the natural logarithm: $\ell(\theta) = \log L(\theta)$
3. Take the derivative with respect to $\theta$: $\frac{d\ell(\theta)}{d\theta}$
4. Set equal to zero and solve: $\frac{d\ell(\theta)}{d\theta} = 0$
5. Verify it's a maximum (check second derivative < 0)

### Score Function and Fisher Information

The **score function** is the derivative of the log-likelihood:

$$
S(\theta) = \frac{\partial \ell(\theta)}{\partial \theta}
$$

The MLE solves $S(\hat{\theta}) = 0$, known as the **score equation** or **likelihood equation**.

## Example 1: Normal Distribution with Known Variance

Suppose we observe $y_1, \ldots, y_n \sim N(\mu, \sigma^2)$ where $\sigma^2$ is known. Find the MLE for $\mu$.

### Step 1: Likelihood Function

$$
L(\mu) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \mu)^2}{2\sigma^2}\right)
$$

### Step 2: Log-Likelihood

$$
\ell(\mu) = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i - \mu)^2
$$

### Step 3: Differentiate

$$
\frac{d\ell(\mu)}{d\mu} = \frac{1}{\sigma^2}\sum_{i=1}^{n}(y_i - \mu)
$$

### Step 4: Set to Zero and Solve

$$
\sum_{i=1}^{n}(y_i - \hat{\mu}) = 0 \implies \hat{\mu} = \frac{1}{n}\sum_{i=1}^{n}y_i = \bar{y}
$$

**Result**: The MLE of $\mu$ is the sample mean $\bar{y}$.

## Example 2: Simple Linear Regression

Consider the simple linear regression model:

$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i, \quad \epsilon_i \sim N(0, \sigma^2)
$$

This means $y_i | x_i \sim N(\beta_0 + \beta_1 x_i, \sigma^2)$.

### Likelihood Function

$$
L(\beta_0, \beta_1, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \beta_0 - \beta_1 x_i)^2}{2\sigma^2}\right)
$$

### Log-Likelihood

$$
\ell(\beta_0, \beta_1, \sigma^2) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i)^2
$$

### Maximization

To maximize with respect to $\beta_0$ and $\beta_1$ (for fixed $\sigma^2$), we need to **minimize**:

$$
\sum_{i=1}^{n}(y_i - \beta_0 - \beta_1 x_i)^2
$$

This is exactly the **sum of squared residuals** (SSR)!

**Key Insight**: Under the assumption of normally distributed errors, Maximum Likelihood Estimation is **equivalent to Ordinary Least Squares**.

### The MLEs

The MLEs are:

$$
\hat{\beta}_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}
$$

$$
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}
$$

$$
\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2
$$

## Properties of Maximum Likelihood Estimators

Under regularity conditions, MLEs have several desirable asymptotic properties:

### 1. Consistency

As $n \to \infty$, $\hat{\theta}_{MLE} \xrightarrow{p} \theta_0$ (converges in probability to the true value)

### 2. Asymptotic Normality

$$
\sqrt{n}(\hat{\theta}_{MLE} - \theta_0) \xrightarrow{d} N(0, I(\theta_0)^{-1})
$$

where $I(\theta)$ is the Fisher Information.

### 3. Asymptotic Efficiency

Among all consistent and asymptotically normal estimators, the MLE has the smallest asymptotic variance (achieves the CramÃ©r-Rao lower bound).

### 4. Invariance Property

If $\hat{\theta}$ is the MLE of $\theta$, then for any function $g(\theta)$, the MLE of $g(\theta)$ is $g(\hat{\theta})$.

## Connection to Least Squares

### When MLE = OLS

For the linear regression model with normal errors:

$$
y_i = \mathbf{x}_i^T\boldsymbol{\beta} + \epsilon_i, \quad \epsilon_i \sim N(0, \sigma^2)
$$

The MLE and OLS estimators are **identical**:

$$
\hat{\boldsymbol{\beta}}_{MLE} = \hat{\boldsymbol{\beta}}_{OLS} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
$$

### When They Differ

- If errors are not normally distributed, MLE and OLS may differ
- OLS doesn't require distributional assumptions for consistency
- MLE requires correct specification of the error distribution
- For non-normal errors, other estimators might be more efficient than OLS

## Practical Implementation

### R Example

```{r}
#| eval: false
#| echo: true

# Generate sample data
set.seed(123)
n <- 100
x <- rnorm(n)
beta0_true <- 2
beta1_true <- 3
sigma_true <- 1.5
y <- beta0_true + beta1_true * x + rnorm(n, 0, sigma_true)

# Define negative log-likelihood function
neg_log_lik <- function(params, x, y) {
  beta0 <- params[1]
  beta1 <- params[2]
  sigma <- params[3]
  
  # Linear prediction
  mu <- beta0 + beta1 * x
  
  # Negative log-likelihood for normal distribution
  nll <- -sum(dnorm(y, mean = mu, sd = sigma, log = TRUE))
  return(nll)
}

# Optimize to find MLEs
library(stats)
initial_params <- c(0, 0, 1)
mle_result <- optim(par = initial_params, 
                    fn = neg_log_lik, 
                    x = x, y = y,
                    method = "BFGS")

# Extract MLEs
beta0_mle <- mle_result$par[1]
beta1_mle <- mle_result$par[2]
sigma_mle <- mle_result$par[3]

# Compare with OLS
ols_model <- lm(y ~ x)

cat("MLE estimates:\n")
cat("Beta0:", beta0_mle, "\n")
cat("Beta1:", beta1_mle, "\n")
cat("Sigma:", sigma_mle, "\n\n")

cat("OLS estimates:\n")
cat("Beta0:", coef(ols_model)[1], "\n")
cat("Beta1:", coef(ols_model)[2], "\n")
cat("Sigma:", summary(ols_model)$sigma, "\n")
```

## Summary

**Maximum Likelihood Estimation** provides a principled framework for parameter estimation:

1. **Intuition**: Find parameters that make observed data most likely
2. **Method**: Maximize the likelihood (or log-likelihood) function
3. **For linear regression with normal errors**: MLE = OLS
4. **Properties**: Consistent, asymptotically normal, and efficient
5. **Flexibility**: Extends naturally to complex models

### Key Takeaways

- MLE requires specifying a full probability model
- The method is widely applicable beyond linear regression
- Understanding MLE provides deeper insight into why OLS works
- The connection between MLE and OLS justifies using OLS under normality

## Further Reading

- For the OLS derivation, see [OLS_Derivation.qmd](OLS_Derivation.qmd)
- For regression introduction, see [Introduction_to_Linear_Regression.qmd](Introduction_to_Linear_Regression.qmd)
- Advanced topics: Generalized Linear Models (GLMs) use MLE when OLS is not appropriate
